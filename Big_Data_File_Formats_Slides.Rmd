---
title: "Big Data File Formats"
author: "Brian High and Miriam Calkins"
date: "![CC BY-SA 4.0](cc_by-sa_4.png)"
output:
  ioslides_presentation:
    fig_caption: yes
    fig_retina: 1
    fig_width: 5
    fig_height: 3
    keep_md: yes
    smaller: yes
    logo: logo_128.png
---

```{r set_knitr_options, echo=FALSE, message=FALSE, warning=FALSE}
suppressMessages(library(knitr))
opts_chunk$set(tidy=FALSE, cache=FALSE, echo=FALSE, message=FALSE, fig.height=4.5)
```

```{r, warning=FALSE, message=FALSE}
# Load pacman into memory, installing as needed.
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(dplyr, ggplot2, tidyr)
```

## Introduction

We define Big Data broadly as "data so big it becomes difficult to manage".

This difficulty can be related to the "Three Vs":

* Volume
* Velocity
* Variety

With a case study, we will show how we managed Big Data Volume in R.

## Case Study: Meteorological Data

We are using climate data from the 
[MACA downscaling](https://climate.northwestknowledge.net/MACA/) datasets. 

* 20 models
* 150 years per model
* 3 "scenarios" per model
* 2 billion rows per model
* 2000 NetCDF files consuming 290 gigabytes (GB)
* would be 3 terabytes (TB) as CSV
* would be 1.5 TB when loaded into memory (RAM)

## File Formats

We compared the following file formats and their performance:

* [RData](https://stat.ethz.ch/R-manual/R-devel/library/base/html/load.html)
* [CSV](https://en.wikipedia.org/wiki/Comma-separated_values)
* [SQLite](https://en.wikipedia.org/wiki/SQLite)
* [MonetDBLite](https://en.wikipedia.org/wiki/MonetDB)
* [XDF](https://docs.microsoft.com/en-us/machine-learning-server/r/concept-what-is-xdf)

## Packages

We will used these R packages:

* [data.table](https://cran.r-project.org/web/packages/data.table/index.html)
* [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html)
* [dbplyr](https://cran.r-project.org/web/packages/dbplyr/index.html)
* [RevoScaleR](https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/revoscaler)
* [DBI](https://cran.r-project.org/web/packages/DBI/index.html)
* [RSQLite](https://cran.r-project.org/web/packages/RSQLite/index.html)
* [MonetDBLite](https://cran.r-project.org/web/packages/MonetDBLite/index.html)
* [pryr](https://cran.r-project.org/web/packages/pryr/index.html)

## Common Approaches to Big Data

We used most of these tools and techniques in our case study.

* Sampling
* Bigger hardware
* Storage (e.g., files or databases) and "chunking"
* Optimized software compilation to support hardware features
* Alternative libraries (packages)
* Alternative interpreters
* Alternative languages

(Modified from [Five ways to handle Big Data in R](https://www.r-bloggers.com/five-ways-to-handle-big-data-in-r/) by Oliver Bracht.)

## Requirements Analysis

To select the best tools for the job, we considered:

* Security/Access Requirements
* Software Requirements
* Storage Requirements
* Memory Requirements
* Processing Requirements
* Performance Requirements

## File Size

The SQL-based formats were about the size as RData, with XDF and GZipped CSV 
much smaller and uncompressed CSV much larger.

```{r}
sz_file_type <- c('CSV', 'GZip-CSV', 'SQLite', 'MonetDBLite', 'XDF', 'RData',  
                  'SQLite', 'MonetDBLite')
sz_gb <- c(145.5, 26.3, 109.4, 72.5, 11, 95.8, 109.4, 96.4)
sz_when <- c('Fresh', 'Fresh', 'Fresh','Fresh', 'Fresh', 'Fresh', 'Used', 'Used')
file_sz_df <- data.frame(file.type=sz_file_type, 
                         file.size=sz_gb,
                         when=sz_when)
p <- ggplot(file_sz_df %>% group_by(file.type) %>% summarize(file.size = min(file.size)), 
            aes(x=reorder(file.type, file.size), y=file.size)) + 
     geom_bar(stat="identity", position=position_dodge()) + 
     labs(title="File Size by File Type", x="File Type", y = "File Size (GB)") + 
     geom_text(aes(label=file.size), vjust=1.6, color="white", size=4) + 
     theme_minimal()
```

```{r, plot_file_size}
suppressWarnings(print(p))
```

## Benchmark Query: SQL

Our benchmark query is a simple filter/group/summarize operation expressed 
in SQL as:

    SELECT scenario, 
           MAX(tasmax) AS maxtmax, 
           MIN(tasmin) AS mintmin  
    FROM metdata 
    WHERE NOT tasmax = 0 AND NOT tasmin = 0 
    GROUP BY scenario;

Where `maxtmax` is the maximum of the daily maximum temperatures and 
`mintmin` is the minimum of the daily minimum temperatures.

## Benchmark Query: _dplyr_ and _data.table_

This is equivalent to this _dplyr_ pipeline:

    df %>% filter(tasmax != 0, tasmin != 0) %>% 
           group_by(scenario) %>% 
           summarise(maxtmax=max(tasmax, na.rm = TRUE), 
                     mintmin=min(tasmin, na.rm = TRUE))

Or this _data.table_ operation:

    dt[tasmax != 0 & tasmin != 0, 
       .(maxtmax=max(tasmax, na.rm = TRUE), 
         mintmin=min(tasmin, na.rm = TRUE)), 
       by = list(scenario)]

## Benchmark Query: _rxSummary_

With XDF, we can use the _rxSummary_ function: 

    rxSummary(~tasmax:scenario + tasmin:scenario, 
              data=xdf.dso, removeZeroCounts = TRUE, 
              summaryStats=c("Max", "Min"))

But to get the output formatted as in the previous examples, we use _dplyr_ 
to reformat it:

    df.rxs$categorical[[1]][c('scenario', 'Max')] %>% 
        full_join(df.rxs$categorical[[2]][c("scenario", 'Min')],
                  by = 'scenario') %>% 
        rename(maxtmax=Max, mintmin=Min) %>% mutate(model=mod)

Where `df.rxs` is the `data.frame` object we used to store the output of 
_rxSummary_.

## On-Disk Queries

MonetDBLite was much faster than SQL. XDF was fast too, but it's query features 
are more limited compared to the expressiveness of SQL.

```{r}
on_disk_file_type <- c('SQLite', 'SQLite', 'MonetDBLite', 'MonetDBLite', 'XDF')
on_disk_query_time <- c(73.1, 72.7, 11.4, 13, 9.6)
on_disk_query_type <- c('SQL', 'dbplyr', 'SQL', 'dbplyr', 'rxSummary')
on_disk_query_df <- data.frame(file.type=on_disk_file_type,
                               query.time=on_disk_query_time,
                               query.type=on_disk_query_type)
p <- ggplot(on_disk_query_df, 
            aes(x=reorder(query.type, query.time), 
                y=query.time)) + 
     geom_bar(stat="identity", position=position_dodge()) + 
     labs(title='"On-Disk" Query Time by File Type', x="File Type", y = "Query Time (minutes)") + 
     theme_minimal() + facet_grid(. ~ file.type) + 
     geom_text(aes(label=query.time), vjust=1.6, color="white", size=4)
```

```{r, plot_on_disk_query_time}
p
```

## File Import Times

MonetDBLite was the fastes to import bay far. SQLite and GZipped CSV took much 
to long to read to be considered competitive with the others.

```{r}
mem_file_type <- c('GZip-CSV', 'SQLite', 'MonetDBLite', 'MonetDBLite', 'XDF', 'XDF', 'RData')
mem_imp_time <- c(466.3, 85.8, 14.7, 3.1, 24.5, 29.1, 27.4)
mem_imp_type <- c('data.table', 'data.table', 'data.table', 'data.frame', 'data.frame', 'data.table', 'data.frame')
mem_imp_size <- c(91.2, 91.2, 91.2, 91.2, 83.6, 83.6, 91.2)
mem_imp_df <- data.frame(file.type=mem_file_type,
                         import.time=mem_imp_time,
                         object.type=mem_imp_type,
                         import.size <- mem_imp_size)
p <- ggplot(mem_imp_df %>% group_by(file.type) %>% summarize(import.time = min(import.time)), 
            aes(x=reorder(file.type, import.time), 
                y=import.time)) + 
     geom_bar(stat="identity", position=position_dodge()) + 
     labs(title="Import Time by File Type", 
          x="File Type", y = "Import Time (minutes)") + theme_minimal() + 
    ylim(0,500) +
    geom_text(aes(label=import.time), vjust=-.5, color="#333333", size=4)
```

```{r, plot_import_time}
p
```

## "In-Memory" Calculation Time

_data.table_ is faster than _dplyr_. Making sure factors are coded as factors 
instead of characters can really help, especially when using _dplyr_. Specify 
column types as you import, as it will take a long time to convert after import.

```{r}
calc_obj_type <- c('data.table', 'dplyr', 'dplyr', 'data.table')
calc_col_type <- c('Factor as Character', 'Factor as Character', 
                   'No Mismatch', 'No Mismatch')
calc_time <- c(3.3, 8.3, 5.3, 2.8)
calc_df <- data.frame(calc.type=calc_obj_type, 
                      column.anomalies=calc_col_type,
                      calc.time=calc_time)

p <- ggplot(calc_df, 
            aes(x=reorder(column.anomalies, calc.time), 
                y=calc.time)) + 
     geom_bar(stat="identity", position=position_dodge()) + 
     labs(title='Calculation Time by Column Type Mismatch', 
          x="Column Type Mismatch", y = "Calculation Time (minutes)") + 
     scale_fill_manual(values=c("#999999", "#E69F00")) + theme_minimal() +
     facet_grid(. ~ calc.type) + 
     geom_text(aes(label=calc.time), vjust=1.6, color="white", size=4)
```

```{r, plot_calc_time}
p
```

## Max and Min Temperatures

It takes about 3 hours and 12 minutes to summarize all of the data:

```{r}
res.df <- read.csv('data.csv', stringsAsFactors = FALSE)

res.df <- res.df %>% gather(t.type, temp, -scenario, -model) %>% 
    mutate(temp = temp - 273.15, 
           t.type=gsub('^(max|min).*$', '\\1', t.type))

p <- ggplot(res.df,
            aes(x=reorder(model, temp),  
                y=temp, fill=t.type)) + 
     geom_bar(stat="identity") + 
     labs(title='Max and Min Temperature by Model', 
          x="Model", y = "Temperature (C)") + 
     theme_minimal() + facet_grid(. ~ scenario) + coord_flip() + 
     theme(axis.text.x=element_text(angle=90,hjust=1)) + 
     guides(fill=guide_legend(title="Temp Extreme"))
```

```{r, max_min_temp, fig.width=6}
p
```

## Summary

By gathering project requirements first, then evaluating various options that
fit those requirements, we were able to find two primary data file types to use:
MonetDBLite and XDF. 

Both will perform similarly in terms of speed, with XDF taking less storage 
space but more time to read into memory. If the operations to be performed 
will consume too much memory, then MonetDBLite will also allow some of the 
calculations to be performed in SQL before loading the entire dataset into 
memory. Or, either file type can be subsetted during import to implement a 
"chunking" approach. So, if minimizing storage consumption is a priority, then 
we may prefer XDF, but if execution time and memory conservation are priorities, 
then we may prefer MonetDBLite.

It is quicker to import our data into a `data.frame` rather than converting to 
_data.table_ during or after import. But using the _data.table_ bracket-notation
for some operations will be faster than performing those same operations with 
_dplyr_ functions. So, if we have lots of operations to perform in memory, it 
may be worth the overhead of converting data frame's to data tables in order 
to use the _data.table_ package.
